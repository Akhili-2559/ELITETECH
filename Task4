from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

# Load model from local folder
model_path = "./gpt2-local"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

prompt = input("Enter a topic to generate text on: ")
results = generator(prompt, max_length=150, num_return_sequences=1)

print("\nGenerated Paragraph:\n")
print(results[0]['generated_text'])
